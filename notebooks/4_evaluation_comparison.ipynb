{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions\n",
    "y_pred_roberta = np.load('../models/transfer_learning/roberta_predictions.npy')\n",
    "y_pred_xlnet = np.load('../models/transfer_learning/xlnet_predictions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the true labels\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\")['sentiment']\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "y_test_numeric = y_test.map(label_map).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RoBERTa\n",
    "accuracy_roberta = accuracy_score(y_test_numeric, y_pred_roberta)\n",
    "precision_roberta = precision_score(y_test_numeric, y_pred_roberta, average='weighted')\n",
    "recall_roberta = recall_score(y_test_numeric, y_pred_roberta, average='weighted')\n",
    "f1_roberta = f1_score(y_test_numeric, y_pred_roberta, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XLNet\n",
    "accuracy_xlnet = accuracy_score(y_test_numeric, y_pred_xlnet)\n",
    "precision_xlnet = precision_score(y_test_numeric, y_pred_xlnet, average='weighted')\n",
    "recall_xlnet = recall_score(y_test_numeric, y_pred_xlnet, average='weighted')\n",
    "f1_xlnet = f1_score(y_test_numeric, y_pred_xlnet, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "report_roberta = classification_report(y_test_numeric, y_pred_roberta)\n",
    "report_xlnet = classification_report(y_test_numeric, y_pred_xlnet)\n",
    "\n",
    "print(\"RoBERTa Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_roberta}\")\n",
    "print(f\"Precision: {precision_roberta}\")\n",
    "print(f\"Recall: {recall_roberta}\")\n",
    "print(f\"F1 Score: {f1_roberta}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_roberta)\n",
    "\n",
    "print(\"XLNet Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_xlnet}\")\n",
    "print(f\"Precision: {precision_xlnet}\")\n",
    "print(f\"Recall: {recall_xlnet}\")\n",
    "print(f\"F1 Score: {f1_xlnet}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['RoBERTa', 'XLNet'],\n",
    "    'Accuracy': [accuracy_roberta, accuracy_xlnet],\n",
    "    'Precision': [precision_roberta, precision_xlnet],\n",
    "    'Recall': [recall_roberta, recall_xlnet],\n",
    "    'F1 Score': [f1_roberta, f1_xlnet]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame for plotting\n",
    "comparison_melted = comparison_df.melt(id_vars=['Model'], var_name='Metric', value_name='Score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=comparison_melted, x='Metric', y='Score', hue='Model')\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
